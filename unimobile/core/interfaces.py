from abc import ABC, abstractmethod
from typing import List, Any, Optional
from unimobile.core.protocol import Action
from unimobile.core.protocol import PerceptionResult, MemoryFragment, FragmentType, PerceptionInput, PlanResult, PlanInput
from unimobile.core.protocol import VerifierInput, VerifierResult
from unimobile.knowledge.base import BaseKnowledgeSource
from unimobile.core.context import EnvironmentInfo
from unimobile.knowledge.formatter import format_knowledge

# ==========================================
# 1. Infrastructure Interfaces
# ==========================================
class BaseLLM(ABC):
    """
    LLM Adapter
    Defined the standards for how to interact with the underlying models (OpenAI, DeepSeek, LocalLLM)
    """
    @abstractmethod
    def generate(self, prompt: str, images: List[str] = None) -> str:
        """generate function

        Args:
            prompt (str): text prompt
            images (List[str], optional): List of image paths. Defaults to None.

        Returns:
            str: The original text generated by the model
        """
        pass

# ==========================================
# 2. Module Interfaces
# ==========================================
class BasePerception(ABC):
    """
    Base interface for all perception modules.

    A perception module observes the agent's environment and produces a structured, agent-readable representation of the current state.

    The environment may include, but is not limited to:
    - Visual GUI state (screenshot, UI layouts)
    - System-level capabilities (eg., exposed app APIs or tools)

    Perception modules are responsible for *observation and description only*.
    They must not perform planning, reasoning, or action execution.
    """
    @abstractmethod
    def perceive(self, perception_input: PerceptionInput) -> PerceptionResult:
        """Perform perception on the current environment state.

        This method is the core entry point of the perception module.
        It consumes raw observation inputs (e.g., screenshots or
        system-provided metadata) and produces a PerceptionResult
        that describes what the agent can currently observe.

        Args:
            perception_input (PerceptionInput): 
            A container of raw observation data.
            It currently includes:
             - screenshot_path: Paths to screenshots
             - width, height: screen dimensions

        Returns:
            PerceptionResult: 
            A unified perception output that currently contain:
             - mode (str): Identifier of the perception strategy name used
             - original_screenshot_path (str): Path to the original screenshot used as perception input.
             - elements (List[Dict[str, Any]]): A list of detected elements on the screen. 
             - metadata (Dict[str, Any]): Auxiliary metadata produced during perception.
             - data (Dict[str, Any]): Structured or global semantic information inferred from the screen.
             - marked_screenshot_path (Optional[str]): Optional path to a visualization image where detected elements are annotated on top of the original screenshot.
             - prompt_representation (str): A textual representation of the perception result that can be directly embedded into an LLM prompt. 
               This field allows the perception module to control how observations are presented to downstream reasoning modules.
             - visual_representations (List[str]): The final image returned to the reasoning component for viewing
        """
        pass

    @abstractmethod
    def _get_prompt_context(self, result: Any) -> str:
        """
        Generate a textual prompt context from the perception result.

        This method defines how the perception output is presented
        to language models. Different perception strategies may choose
        different prompt styles, such as:
        - Lists or tables of detected UI elements
        - Natural language summaries of the screen
        - Enumerations of available system APIs or tools
        - Hybrid descriptions combining visual and declarative information

        Args:
            result (Any): Any

        Returns:
            str: A human-readable textual description of the current environment, suitable for direct inclusion in an LLM prompt.
        
        """
        pass

class BaseMemory(ABC):
    def __init__(self, knowledge_source: BaseKnowledgeSource = None):
        self.knowledge_source = knowledge_source
        self.knowledge_buffer: List[MemoryFragment] = []

    @abstractmethod
    def add(self, fragment: MemoryFragment):
        pass
    
    @abstractmethod
    def get_working_context(self) -> List[MemoryFragment]:
        """
        The return value must be a list of MemoryFragments.
        """
        pass

    # Slow Path
    def load_knowledge(self, query: str):
        source = self.knowledge_source

        if not source:
            return
    
        self.knowledge_buffer = []

        docs = source.search_docs(query)

        for doc in docs:
            formatted_content = format_knowledge(doc)

            frag = MemoryFragment(
                role="system",
                type=FragmentType.RAG_DOC,
                content=formatted_content,
                metadata={"category": doc.category}
            )

            self.knowledge_buffer.append(frag)

    # Fast Path
    def retrieve_experience(self, screenshot_path: str, task: str) -> Optional[Action]:
        if not self.knowledge_source:
            return None
            
        return self.knowledge_source.match_trace(screenshot_path, task)

    @abstractmethod
    def clear(self):
        """clear memory"""
        pass

class BasePlanner(ABC):
    def __init__(self, llm_client: Any, knowledge_source: BaseKnowledgeSource = None, env_info: EnvironmentInfo = None):
        self.llm = llm_client
        self.knowledge_source = knowledge_source
        self.env = env_info
    @abstractmethod
    def make_plan(self, plan_input: PlanInput) -> PlanResult:
        """
        :param task: 
        :return:
        """
        pass

class BaseReason(ABC):
    def __init__(self, llm_client: Any, env_info: EnvironmentInfo = None):
        self.llm = llm_client
        self.env = env_info
    @abstractmethod
    def think(self, 
              task: str, 
              plan: PlanResult, 
              perception_result: PerceptionResult, 
              memory_context: List[MemoryFragment]) -> Action:
        pass

class BaseVerifier(ABC):
    @abstractmethod
    def verify(self, input_data: VerifierInput) -> VerifierResult:
        pass

class BasePlannerParser(ABC):
    @abstractmethod
    def parse(self, response: str, **kwargs) -> PlanResult:
        pass

class BaseActionParser(ABC):
    @abstractmethod
    def parse(self, response: str, metadata: dict) -> Action:
        raise NotImplementedError

# ==========================================
# 3. Top-level Interface
# ==========================================
class BaseAgent(ABC):
    """
    All agents whether Modular or externally integrated must comply with this standard
    Runner only recognizes this interface.
    """
    @abstractmethod
    def step(self, screenshot_path: str, width: int, height: int) -> Action:
        pass
    
    @abstractmethod
    def reset(self, task: str):
        pass
